{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd0d6279",
      "metadata": {
        "id": "dd0d6279"
      },
      "source": [
        "# Web Scraping con Python\n",
        "**Nombre del estudiante:** Quispe Sucullani Jose David\n",
        "**Curso:** Inteligencia Artificial II\n",
        "**Práctico:** 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbb6c389",
      "metadata": {
        "id": "bbb6c389"
      },
      "source": [
        "## Introducción\n",
        "El Web Scraping es una técnica ampliamente utilizada en el campo de la inteligencia artificial, ciencia de datos y minería web. Permite la recolección automática de datos desde páginas web, sin depender de interfaces gráficas o APIs. Este proceso es esencial cuando se necesita acceder a información estructurada o no estructurada en la web para tareas de análisis, predicción o entrenamiento de modelos.\n",
        "\n",
        "En este práctico se desarrollará una herramienta en Python que permite extraer información textual e imágenes desde una página web pública. Se explicarán detalladamente las herramientas utilizadas, el procedimiento, así como las ventajas y desventajas de esta técnica."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e15bbdc3",
      "metadata": {
        "id": "e15bbdc3"
      },
      "source": [
        "## Herramientas Utilizadas\n",
        "- **requests**: Para realizar peticiones HTTP y obtener el contenido HTML de la página.\n",
        "- **BeautifulSoup (bs4)**: Para analizar y recorrer el contenido HTML de la página.\n",
        "- **os**: Para la gestión de carpetas y archivos en el sistema.\n",
        "- **urllib.parse.urljoin**: Para resolver correctamente las URLs relativas de imágenes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8068e4ce",
      "metadata": {
        "id": "8068e4ce"
      },
      "source": [
        "## Instalación de Librerías\n",
        "Antes de comenzar, es necesario asegurarse de que las bibliotecas `requests` y `beautifulsoup4` estén instaladas. Esto puede hacerse ejecutando la siguiente celda:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b36a1d99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b36a1d99",
        "outputId": "02f8eaf9-f1b7-48ed-949a-99f9a11bf669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035c9197",
      "metadata": {
        "id": "035c9197"
      },
      "source": [
        "## Importación de Librerías\n",
        "Se importan las librerías necesarias para desarrollar el programa de scraping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "39fbd44f",
      "metadata": {
        "id": "39fbd44f"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from urllib.parse import urljoin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f8ccd4",
      "metadata": {
        "id": "c3f8ccd4"
      },
      "source": [
        "## Procedimiento Detallado\n",
        "1. Se realiza una petición HTTP a la página web de destino utilizando `requests.get()`.\n",
        "2. Se utiliza `BeautifulSoup` para interpretar el código HTML de la página.\n",
        "3. Se extraen todos los textos que se encuentren dentro de las etiquetas `<p>` (párrafos).\n",
        "4. Se buscan todas las etiquetas `<img>` para identificar imágenes.\n",
        "5. Las imágenes se descargan en formato binario y se guardan localmente.\n",
        "6. Todo el contenido (texto e imágenes) se almacena en una carpeta llamada `scraped_data`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd736d61",
      "metadata": {
        "id": "cd736d61"
      },
      "source": [
        "## Función Principal para el Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2a096fd3",
      "metadata": {
        "id": "2a096fd3"
      },
      "outputs": [],
      "source": [
        "def scrape_website(url, output_folder='scraped_data'):\n",
        "    # Realizar la solicitud HTTP a la página\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Crear la carpeta donde se guardarán los datos extraídos\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Extraer textos desde etiquetas <p>\n",
        "    paragraphs = soup.find_all('p')\n",
        "    with open(os.path.join(output_folder, 'contenido_textual.txt'), 'w', encoding='utf-8') as f:\n",
        "        for p in paragraphs:\n",
        "            texto = p.get_text(strip=True)\n",
        "            if texto:\n",
        "                f.write(texto + '\\n')\n",
        "\n",
        "    # Extraer imágenes desde etiquetas <img>\n",
        "    images = soup.find_all('img')\n",
        "    for i, img in enumerate(images):\n",
        "        img_url = img.get('src')\n",
        "        if img_url:\n",
        "            img_url = urljoin(url, img_url)\n",
        "            try:\n",
        "                img_data = requests.get(img_url).content\n",
        "                with open(os.path.join(output_folder, f'imagen_{i+1}.jpg'), 'wb') as f:\n",
        "                    f.write(img_data)\n",
        "            except Exception as e:\n",
        "                print(f\"No se pudo descargar la imagen: {img_url}\")\n",
        "    print(f\"Extracción finalizada. Archivos guardados en la carpeta '{output_folder}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bedffe51",
      "metadata": {
        "id": "bedffe51"
      },
      "source": [
        "## Ejecución de la Función con una URL de Prueba\n",
        "Para efectos de prueba, se puede utilizar una página con texto e imágenes como un artículo de Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0f23116e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f23116e",
        "outputId": "7cd93e78-4317-42ca-e1fb-5053492b62fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracción finalizada. Archivos guardados en la carpeta 'scraped_data'.\n"
          ]
        }
      ],
      "source": [
        "scrape_website('https://es.wikipedia.org/wiki/Python')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc1c6c3",
      "metadata": {
        "id": "acc1c6c3"
      },
      "source": [
        "## Verificación de los Archivos Guardados\n",
        "Después de ejecutar el scraping, se puede listar el contenido de la carpeta donde se guardaron los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "57339a06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57339a06",
        "outputId": "f5b4bd4f-33e4-48da-f996-4b474ca321a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos guardados en la carpeta scraped_data:\n",
            "- imagen_16.jpg\n",
            "- imagen_2.jpg\n",
            "- imagen_15.jpg\n",
            "- imagen_11.jpg\n",
            "- imagen_14.jpg\n",
            "- imagen_7.jpg\n",
            "- imagen_6.jpg\n",
            "- imagen_20.jpg\n",
            "- imagen_13.jpg\n",
            "- imagen_18.jpg\n",
            "- imagen_8.jpg\n",
            "- imagen_10.jpg\n",
            "- imagen_12.jpg\n",
            "- imagen_21.jpg\n",
            "- contenido_textual.txt\n",
            "- imagen_17.jpg\n",
            "- imagen_9.jpg\n",
            "- imagen_1.jpg\n",
            "- imagen_5.jpg\n",
            "- imagen_19.jpg\n",
            "- imagen_4.jpg\n",
            "- imagen_22.jpg\n",
            "- imagen_3.jpg\n"
          ]
        }
      ],
      "source": [
        "print('Archivos guardados en la carpeta scraped_data:')\n",
        "for archivo in os.listdir('scraped_data'):\n",
        "    print('-', archivo)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb4bb6f",
      "metadata": {
        "id": "5bb4bb6f"
      },
      "source": [
        "## Ventajas del Web Scraping\n",
        "- Permite recolectar grandes volúmenes de información de manera automática.\n",
        "- Es útil cuando una página no ofrece APIs para acceder a sus datos.\n",
        "- Puede ser adaptado a múltiples formatos de datos: texto, imágenes, tablas, etc.\n",
        "- Facilita análisis comparativos, monitoreo de precios, o recopilación de información pública.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fb70e3",
      "metadata": {
        "id": "b2fb70e3"
      },
      "source": [
        "## Desventajas y Consideraciones Éticas\n",
        "- Muchas páginas web bloquean el acceso automatizado a través del archivo `robots.txt`.\n",
        "- El scraping excesivo puede afectar el rendimiento del servidor objetivo.\n",
        "- Es posible infringir términos y condiciones si no se tiene autorización explícita.\n",
        "- No todas las páginas web permiten el acceso a datos protegidos mediante JavaScript o autenticación.\n",
        "- Se deben tomar medidas para evitar la sobrecarga del sitio, como agregar `time.sleep()` entre peticiones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6edc6acf",
      "metadata": {
        "id": "6edc6acf"
      },
      "source": [
        "## Conclusión\n",
        "El Web Scraping es una herramienta poderosa para la obtención de datos en el contexto de inteligencia artificial. En este trabajo práctico se implementó un script básico para obtener información textual e imágenes desde una página web pública. El estudiante aprendió a utilizar bibliotecas especializadas de Python y a estructurar el flujo de trabajo de recolección de datos, reconociendo sus beneficios, limitaciones y buenas prácticas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}